{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7a4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fraud_detection.viz.model_plots as viz\n",
    "from fraud_detection.models.pipeline import build_pipeline\n",
    "from fraud_detection.models.train import train_and_evaluate\n",
    "from fraud_detection.models.compare import score_models, compare_models,promote_best_model\n",
    "from fraud_detection.core.settings import settings\n",
    "from fraud_detection.data.loader import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65765ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_original = DataHandler.from_registry(\"DATA\", \"processed_dir\", \"test_original.parquet\").load()\n",
    "train_original = DataHandler.from_registry(\"DATA\", \"processed_dir\", \"train_original.parquet\").load()\n",
    "train_resampled = DataHandler.from_registry(\"DATA\", \"processed_dir\", \"train_resampled.parquet\").load()\n",
    "\n",
    "FEATURES = settings.get(\"features\")\n",
    "TARGET = FEATURES[\"target\"]\n",
    "\n",
    "X_train_orig = train_original.drop(columns=[TARGET])\n",
    "y_train_orig = train_original[TARGET]\n",
    "\n",
    "X_train_res = train_resampled.drop(columns=[TARGET])\n",
    "y_train_res = train_resampled[TARGET]\n",
    "\n",
    "X_test = test_original.drop(columns=[TARGET])\n",
    "y_test = test_original[TARGET]\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "\n",
    "def get_profile(profile_name: str) -> dict:\n",
    "    profile = settings.get(\"profiles\", {}).get(profile_name)\n",
    "    if not profile:\n",
    "        raise ValueError(f\"Profile '{profile_name}' not found in settings.\")\n",
    "    return profile\n",
    "PROFILE_NAME = \"credit_card\"\n",
    "profile = get_profile(PROFILE_NAME)\n",
    "\n",
    "# Registry\n",
    "experiment_name = profile[\"registry\"][\"experiment_name\"]\n",
    "registered_model_name = profile[\"registry\"][\"registered_model_name\"]\n",
    "\n",
    "results = {}\n",
    "run_ids = {}\n",
    "thresholds = {}\n",
    "pipelines = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Baseline models (original data)\n",
    "# ---------------------------\n",
    "baseline_models = {\n",
    "    \"Logistic Regression\": {\"pipeline\": build_pipeline(\"logistic_regression\"), \"threshold_metric\": \"f1\"},\n",
    "}\n",
    "\n",
    "for model_name, cfg in baseline_models.items():\n",
    "    print(f\"\\n=== Training {model_name} (baseline) ===\")\n",
    "    pipe, metrics, threshold, run_id = train_and_evaluate(\n",
    "        pipeline=cfg[\"pipeline\"],\n",
    "        X_train=X_train_orig,\n",
    "        y_train=y_train_orig,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        model_name=model_name,\n",
    "        profile_name=PROFILE_NAME,\n",
    "    )\n",
    "    results[model_name] = metrics\n",
    "    run_ids[model_name] = run_id\n",
    "    thresholds[model_name] = threshold\n",
    "    pipelines[model_name] = pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Ensemble / more complex models (resampled data)\n",
    "# ---------------------------\n",
    "ensemble_models = {\n",
    "    \"Random Forest\": {\"pipeline\": build_pipeline(\"random_forest\"), \"threshold_metric\": \"f1\"},\n",
    "    \"XGBoost\": {\"pipeline\": build_pipeline(\"xgboost\"), \"threshold_metric\": \"f1\"},\n",
    "    \"LightGBM\": {\"pipeline\": build_pipeline(\"lightgbm\"), \"threshold_metric\": \"f1\"},\n",
    "}\n",
    "\n",
    "for model_name, cfg in ensemble_models.items():\n",
    "    print(f\"\\n=== Training {model_name} (resampled) ===\")\n",
    "    pipe, metrics, threshold, run_id = train_and_evaluate(\n",
    "        pipeline=cfg[\"pipeline\"],\n",
    "        X_train=X_train_res,\n",
    "        y_train=y_train_res,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        model_name=model_name,\n",
    "        profile_name=PROFILE_NAME,\n",
    "    )\n",
    "    results[model_name] = metrics\n",
    "    run_ids[model_name] = run_id\n",
    "    thresholds[model_name] = threshold\n",
    "    pipelines[model_name] = pipe\n",
    "\n",
    "# ---------------------------\n",
    "# Compare & score models\n",
    "# ---------------------------\n",
    "print(\"\\n=== Raw metrics comparison ===\")\n",
    "print(compare_models(results))\n",
    "\n",
    "print(\"\\n=== Profile-based scoring ===\")\n",
    "scored_df = score_models(results, PROFILE_NAME)\n",
    "print(scored_df)\n",
    "\n",
    "# ---------------------------\n",
    "# Promote best model\n",
    "# ---------------------------\n",
    "best_model_name = scored_df.index[0]\n",
    "best_metrics = scored_df.iloc[0].to_dict()\n",
    "run_id_to_promote = run_ids[best_model_name]\n",
    "promotion_msg = promote_best_model(profile_name=PROFILE_NAME, run_id=run_id_to_promote)\n",
    "\n",
    "print(f\"\\nPromotion result: {promotion_msg}\")\n",
    "print(f\"Best model selected: {best_model_name}\")\n",
    "print(f\"Metrics: {best_metrics}\")\n",
    "print(f\"Thresholds: {thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ab002",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "\n",
    "for model_name, pipe in pipelines.items():\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= thresholds[model_name]).astype(int)\n",
    "\n",
    "    predictions[model_name] = {\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_proba\": y_proba,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0704b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, data in predictions.items():\n",
    "    y_pred = data[\"y_pred\"]\n",
    "    y_proba = data[\"y_proba\"]\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=3,\n",
    "        figsize=(20, 5),\n",
    "    )\n",
    "\n",
    "    # 1️⃣ Confusion Matrix\n",
    "    viz.plot_confusion_matrix(\n",
    "        y_true=y_test.values,\n",
    "        y_pred=y_pred,\n",
    "        title=\"Confusion Matrix\",\n",
    "        ax=axes[0],\n",
    "    )\n",
    "\n",
    "    # 2️⃣ Precision–Recall Curve\n",
    "    viz.plot_precision_recall_curve(\n",
    "        y_true=y_test.values,\n",
    "        y_proba=y_proba,\n",
    "        model_name=model_name,\n",
    "        ax=axes[1],\n",
    "    )\n",
    "\n",
    "    # 3️⃣ ROC Curve\n",
    "    viz.plot_roc_curve(\n",
    "        y_true=y_test.values,\n",
    "        y_proba=y_proba,\n",
    "        model_name=model_name,\n",
    "        ax=axes[2],\n",
    "    )\n",
    "\n",
    "    fig.suptitle(model_name, fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4️⃣ Classification Report\n",
    "    report_df = viz.get_classification_report_df(\n",
    "        y_true=y_test.values,\n",
    "        y_pred=y_pred,\n",
    "    )\n",
    "    print(report_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
